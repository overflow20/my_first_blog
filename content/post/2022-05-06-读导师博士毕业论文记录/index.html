---
title: 读导师博士毕业论文记录
author: zyy
date: '2022-05-06'
slug: 读导师博士毕业论文记录
categories: []
tags: []
---



<div id="chap1-引言" class="section level1">
<h1>chap1 引言</h1>
<p>模型平均，顾名思义就是把来自不同模型的估计或者预测通过一定的权重平均起来，包括组合估计和组合预测。</p>
<div id="模型平均的动机" class="section level4">
<h4>模型平均的动机</h4>
<p>模型选择的缺陷：</p>
<ol style="list-style-type: decimal">
<li>不稳健。</li>
<li>遗失有用的信息。</li>
<li>高风险。</li>
<li>忽视模型选择过程所带来的不确定性。</li>
<li>目标偏离。</li>
</ol>
<p>模型平均可以避免这些缺陷。</p>
</div>
<div id="模型平均的起源发展和应用" class="section level4">
<h4>模型平均的起源、发展和应用</h4>
<ol style="list-style-type: decimal">
<li>起源：Barnard(1963)</li>
<li>发展：BMA(Bayesian Model Averaging)与FMA(Frequentist Model Averaging)</li>
</ol>
<p>BMA：设定待组合模型的先验概率和各个模型中参数的先验分布，然后用经典的贝叶斯方法进行统计推断。</p>
<p>FMA:</p>
<ol style="list-style-type: decimal">
<li><p>基于信息准则的加权方法。
S-AIC 与 S-BIC,其组合权重为：
<span class="math inline">\(w_k = \dfrac{exp(-xIC_k/2)}{\sum_{k&#39;}exp(-xIC_{k&#39;}/2)}\)</span></p></li>
<li><p>自适应方法。
在一定条件下，它们所得的模型平均估计的风险被常数倍的模型选择估计最小风险加一个较小惩罚所界住。</p></li>
<li><p>渐进最优理论。
Hansen(2007)提出MMA(Mallows Model Averaging)估计，
证明了其是渐进最优的，即当把权重限定在一定集合中时，MMA估计的二次损失依概率渐进等价于用该集合中权重加权的模型平均估计的二次损失的下确界。
但存在两个较强的假设：</p>
<ol style="list-style-type: decimal">
<li>权重必须限定在一个特殊的离散集合中。</li>
<li>待组合模型是嵌套的。</li>
</ol></li>
<li><p>极限分布理论。
Hjort &amp; Claeskens(2003)在极大似然估计的大样本理论基础上构建了模型平均估计的极限分布理论，同时详细阐述了在使用模型选择估计时忽视模型选择过程本身所带来的不确定性的后果，并提出了解决方案。</p></li>
</ol>
</div>
<div id="本文的创新工作和结构安排" class="section level4">
<h4>本文的创新工作和结构安排</h4>
</div>
</div>
<div id="chap2-线性估计的模型平均方法" class="section level1">
<h1>chap2 线性估计的模型平均方法</h1>
<p>本章是关于线性估计的模型平均方法。
#### 2.1 前言
随机样本<span class="math inline">\(y_i,i=1,\cdots,n\)</span>, DGP(Data Generating Process):
<span class="math display">\[\begin{equation}
y_i = \mu_i+e_i = f(x_i)+e_i
\end{equation}\]</span>
<span class="math inline">\(y = (y_1,\cdots,y_n)&#39;,X = (x_1&#39;,\cdots,x_n&#39;)&#39;,\mu = (\mu_1,\cdots,\mu_n)&#39;,e=(e_1,\cdots,e_n)&#39;\)</span>,则有：
<span class="math display">\[\begin{equation}
E(y|X) = \mu, \Omega = Var(e|X)
\end{equation}\]</span></p>
<p>考虑<span class="math inline">\(\mu\)</span>的<span class="math inline">\(K\)</span>种线性估计<span class="math inline">\(\{\hat{\mu}_{(1)},\cdots,\hat{\mu}_{(K)}\}\)</span>,<span class="math inline">\(K\)</span>可以随样本量<span class="math inline">\(n\)</span>变化，<span class="math inline">\(\hat{\mu}_{(K)} = P_ky\)</span>,<span class="math inline">\(P_k\)</span>只依赖<span class="math inline">\(X\)</span>.</p>
<p>权重向量<span class="math inline">\(\omega = (\omega_1,\cdots,\omega_K)&#39;\)</span>,并限制在如下权重集合种：
<span class="math display">\[H_n = \{\omega\in [0,1]^K: \sum\limits_{k=1}^{K}\omega_k = 1 \}\]</span></p>
<p>所以<span class="math inline">\(\mu\)</span>的模型平均估计为:
<span class="math display">\[\hat\mu(\omega) = \sum\limits_{k=1}^{K}\omega_k\hat\mu_{(k)}=\sum\limits_{k=1}^{K}\omega_kP_ky=P(\omega)y\]</span></p>
<p>记<span class="math inline">\(I_n\)</span>为<span class="math inline">\(n\)</span>维单位阵。令<span class="math inline">\(A_k = I_n-P_k\)</span>,<span class="math inline">\(A(\omega)=\sum\limits_{k=1}^{K}\omega_kA_k\)</span>,Mallows准则为：
<span class="math display">\[C_n(\omega) = ||A(\omega)y||^2+2\sigma^2tr(P(\omega))\]</span></p>
<p>因此通过极小化Mallows准则所得到的权重为：
<span class="math display">\[\hat\omega = argmin_{w\in H_n}C_n(\omega)\]</span></p>
<p>定义<span class="math inline">\(\hat\mu(\omega)\)</span>的二次损失为
<span class="math display">\[L_n(\omega)=||\hat\mu(\omega)-\mu||^2\]</span></p>
<p>条件风险为<span class="math inline">\(R_n(\omega) = E(L_n(\omega)|X)\)</span>,所以有：
<span class="math display">\[R_n(\omega) = ？\]</span></p>
<p>令<span class="math inline">\(W\)</span>是一个权重集合，模型平均估计<span class="math inline">\(\hat\mu(\hat\omega)\)</span>是基于集合<span class="math inline">\(W\)</span>的渐进最优性是指：
<span class="math display">\[\dfrac{L_n(\hat\omega)}{inf_{\omega \in W}L_n(\omega)}\stackrel{P}{\longrightarrow} 1\]</span></p>
<div id="mma方法" class="section level4">
<h4>MMA方法</h4>
<div id="概述" class="section level6">
<h6>概述</h6>
<p>在<a href="https://www.ssc.wisc.edu/~bhansen/papers/mallows.pdf">Hansen(2007)</a>中：</p>
<p>考虑<span class="math inline">\(K\)</span>个近似模型，其中第<span class="math inline">\(k\)</span>个近似模型为：
<span class="math display">\[y_i = \sum\limits_{j=1}^{\phi_k}\theta_jx_{ij}+e_i, i=1,\cdots,n\]</span>
使用前<span class="math inline">\(\phi_k\)</span>个<span class="math inline">\(x_{ij}\)</span>作为解释变量，<span class="math inline">\(0&lt;\phi_1&lt;\cdots&lt;\phi_K\)</span>,可记为：
<span class="math display">\[y=X_k\Theta_k+e\]</span></p>
</div>
<div id="mma估计的渐进最优性" class="section level6">
<h6>MMA估计的渐进最优性</h6>
<p><img src="1.jpg" /></p>
<p>这个定理真复杂。。。</p>
<p>但接着解决了MMA估计在连续权重集合和非嵌套模型框架下的渐进最优性。</p>
</div>
</div>
<div id="lma方法" class="section level4">
<h4>LMA方法</h4>
<p>引入了LMA估计</p>
</div>
<div id="jma方法" class="section level4">
<h4>JMA方法</h4>
<p>允许数据相依，限定权重在一般的连续集合情况下，证明了JMA估计的渐进最优性。</p>
</div>
</div>
